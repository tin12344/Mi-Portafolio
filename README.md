# My Portfolio


## ApiChatGpt

The objective of this project was to use the API of chat gpt, in this way we can create our own chat gtp with some more functions like generating images, receiving larger promps. This project was made with React.js and the use of GraphQl that was implemented by Nodejs.

## SocialHub

This project had the objective to use the APIs of some social medias, in this way we can create post and publish at the moment or we can program the post for an especific hour and day. The APIs were of LinkedIn, Reddit and Twitter now known like X. The FrontEnd was made with React.js and the BackEnd was made with Laravel.


## UnitTest

This project had the objective to create unit test of a basic program to know how you can implement unittest on an application, the project was created on Phyton.

## Data Science

In the data science folder are my practice works that I have worked through the Data Science Program, here we learned about some classification algorithms such as random forest, also some ways to treat outliers or missing values, using techniques like one hot encoding, temporal series, investigate linear regression to use supervised models and no supervised models like DBSCAN that the objective was known how many clusters where the ideal for the dataset.

## DepFinder

For the app DepFinder itâ€™s a web and application with the objective to publish your apartment, house or room to other people and this way they can rent it, the application has maps to search apartments near the location, can use filters to make easier the search, has favorite section where the user can add all the publications that is interested on. The backend was develop with PHP Laravel and the frontend with Flutter.  The url to visit the page is https://depfinder.greencodecr.com/home .

## Big Data

For the folder of Big Data folder are my works that are related to big date, that I have worked through the Data Science Program. I learned how to use pyspark to understand how does map reduce works, also running unit test to see the funcionality of the functions that were used on the work, also to treat big quantity of data and to relate 2 datasets to have a richer dataset, also I had to save the data on a postgreSQL database to be consumed later on.

## Log

For the log folder, this is a small practice using pyspark for analyzing logs, doing different types of analysis like the perfomance of the end point response and the traffic, also the security, looking fo ddos attacks, error codes and if there were any ip anomalous behavior.
